<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Kimberly the Geek">
    <meta name="description" content="Kimberly Sereduck&#39;s Blog">
    <meta name="keywords" content="blog,developer,personal">

    <base href="http://www.kimberlythegeek.com">
    <title>
  Scrape Stack Overflow Jobs Using Scrapy | Python Tutorial · Kimberly the Geek
</title>

    <link rel="canonical" href="http://www.kimberlythegeek.com/posts/scrape-stack-overflow-jobs/">

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.1.0/css/all.css" integrity="sha384-lKuwvrZot6UHsBSfcMvOkWwlCMgc0TaWr+30HWe3a4ltaBwTZhyTEggF5tJv8tbt" crossorigin="anonymous">
    <link  rel="stylesheet" href="https://fonts.googleapis.com/css?family=Fira+Mono:400,700">
    <link rel="stylesheet" href="//cdn.rawgit.com/necolas/normalize.css/master/normalize.css">
    <link rel="stylesheet" href="http://www.kimberlythegeek.com/css/style.min.css">

    

    
      <link rel="stylesheet" href="http://www.kimberlythegeek.com/css/style.css">
    

    <link rel="icon" type="image/png" href="http://www.kimberlythegeek.com/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="http://www.kimberlythegeek.com/images/favicon-16x16.png" sizes="16x16">

    

    <meta name="generator" content="Hugo 0.42.1" />
  </head>

  <body class="">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="http://www.kimberlythegeek.com">
      Kimberly the Geek
    </a>
    
    <ul class="navigation-list  float-right ">
      
      <li class="navigation-item">
        <a class="navigation-link" href="http://www.kimberlythegeek.com/posts/">Blog</a>
      </li>
      
      <li class="navigation-item">
        <a class="navigation-link" href="http://www.kimberlythegeek.com/about/">About</a>
      </li>
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
  <article>
    <header>
      <h1 class="title">Scrape Stack Overflow Jobs Using Scrapy | Python Tutorial</h1>
      <h2 class="date">February 10, 2017</h2>

      
    </header>

    <div class="inner-post-nav">
<a class="internal-link" id="wth-is-scraping">1. What the Hell is Scraping, Anyway?</a><br />
<a class="internal-link" id="install-scrapy">2. Install Scrapy</a><br />
<a class="internal-link" id="write-yo-script">3. Write Your First Script</a><br />
<a class="internal-link" id="work-it-good">4. Put It To Work!</a>
</div>
<h5>For this tutorial, we're going to write a Web Spider to scrape Stack Overflow Jobs.</h5>
<h2 class="wth-is-scraping">1. What the Hell is Scraping, Anyway?</h2>
<blockquote><p>
Web scraping (web harvesting or web data extraction) is data scraping used for extracting data from websites. Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser.</p>
<p>While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler.<sup><a href="https://en.wikipedia.org/wiki/Web_scraping" target="_blank">[1]</a></sup>
</p></blockquote>
<p>What's a "Web Spider"?</p>
<blockquote><p>
A Web crawler, sometimes called a spider, is an Internet bot which systematically browses the World Wide Web, typically for the purpose of Web indexing (web spidering).<sup><a href="https://en.wikipedia.org/wiki/Web_crawler" target="_blank">[2]</a></sup></p></blockquote>
<p><!--more--></p>
<h2 class="install-scrapy">2. Install Scrapy</h2>
<p>We'll be using <a href="https://scrapy.org/" target="_blank">Scrapy</a> to write our first website scraping script. There is extensive documentation for Scrapy <a href="https://doc.scrapy.org/en/latest/" target="_blank">here</a>.</p>
<p>Before we begin, ensure that you set up a <a href="posts/python-virtual-environments/" target="_blank">Python virtual environment</a> for this project.</p>
<p>There is also a detailed <a href="https://doc.scrapy.org/en/1.3/intro/install.html" target="_blank">Installation Guide</a> on the Scrapy docs website.</p>
<p>Once you've done that, activate your virtual environment and install Scrapy:</p>
<pre>
$ cd scrapy-project
$ venv-activate
<strong>(scrapy-project-venv)</strong> $ pip install scrapy
</pre>
<p>From there, we can create our Scrapy project:</p>
<pre>
$ scrapy startproject scrapy-project
</pre>
<h2 class="write-yo-script">3. Write Your First Script</h2>
<p>If you're new to Python or Scrapy (like me) I'd recommend taking a look at the the <a href="https://doc.scrapy.org/en/1.3/intro/tutorial.html" target="_blank">Scrapy Tutorial</a> in the documentation.</p>
<p>Create a new file in your project directory, under <code>[PROJECT_NAME]/spiders/</code> named <strong>stackoverflow_spider.py</strong>.</p>
<p>The first part of our file will be declaring our class:</p>
<p>[python]<br />
import scrapy</p>
<p>class StackOverflowSpider(scrapy.Spider):<br />
    name = 'stackoverflow'<br />
    start_urls = [<br />
        'http://stackoverflow.com/jobs?l=Remote&d=20&u=Miles&sort=p'<br />
    ]<br />
[/python]</p>
<ol>
<li>First we have <strong>import scrapy</strong>.</li>
<li>Give your class a relevant name, i.e. <strong>StackOverflowSpider</strong>.</li>
<li>The <code>name</code> class variables is the name you will use to tell Scrapy to run your web crawler.</li>
<li>Scrapy will automatically run the crawler on each URL in the <code>start_urls</code> array.</li>
</ol>
<p>The URL I've used here is searching only for remote jobs, and sorting them by date.</p>
<p>You can also add queries to search for specific types of jobs.</p>
<p>The next parse is to define the <strong>parse</strong> function, which your class will call to handle the response from your URL(s):</p>
<p>[python]<br />
def parse(self, response):<br />
  for result in response.css('.jobs div.-job'):<br />
    yield {<br />
      'company': result.css('.-job-info .employer::text').extract_first(),<br />
      'title': result.css('.-job-info .-title .job-link::text').extract_first(),<br />
      'date': result.css('.-job-info .posted::text').extract_first(),<br />
      'url': result.css('.-job-info .-title .job-link::attr(href)').extract_first(),<br />
    }<br />
[/python]</p>
<ul>
<li>The <code>response</code> variable will hold all of the HTML from the URL(s) called.</li>
<li>Here we have a <strong>for loop</strong> to iterate through each result, defined by its CSS selector: <code>.-job-info .employer</code></li>
<li>We use <code>::text</code> to only extract the text within the tags; otherwise it will grab all of the HTML within the element.</li>
<li>We're using <code>extract_first()</code> instead of <code>extract()</code>, so that it doesn't put the result into an array.</li>
</ul>
<blockquote><p>
To learn more about CSS and XPATH selectors in Scrapy, read <a href="https://doc.scrapy.org/en/1.3/topics/selectors.html" target="_blank">this section</a> of the docs.</p></blockquote>
<p>For this spider, we don't actually need to visit each listing, but you can<a href="https://doc.scrapy.org/en/1.3/intro/tutorial.html#more-examples-and-patterns" target="_blank">learn how here</a>.</p>
<h2 class="work-it-good">4. Put It To Work!</h2>
<p>So let's run our crawler and see what we get:</p>
<pre>$ scrapy crawl stackoverflow -o stackoverflow.json</pre>
<p>The <code>-o</code> flag tells Scrapy to output the results to a file, <strong>stackoverflow.json</strong>.</p>
<p><img src="images/so-crawler.jpg" alt="scrapy crawler python json scrape stack overflow jobs" width="525" height="295" /></p>
<p>Your file should look something like this:<br />
[json]<br />
[<br />
{<br />
  "date": "\r\n                        yesterday\r\n                    ",<br />
  "url": "http://stackoverflow.com/jobs/125185/software-engineer-cloud-technology-partners",<br />
  "company": "\r\n                        Cloud Technology Partners\r\n                    ",<br />
  "title": "Software Engineer"<br />
},<br />
{<br />
  "date": "\r\n                        3 hours ago\r\n                    ",<br />
  "url": "http://stackoverflow.com/jobs/132800/dev-ops-engineer-confluence",<br />
  "company": "\r\n                        Confluence\r\n                    ",<br />
  "title": "Dev Ops Engineer"<br />
},<br />
{<br />
  "date": "\r\n                        5 hours ago\r\n                    ",<br />
  "url": "http://stackoverflow.com/jobs/119653/lead-javascript-developer-planity",<br />
  "company": "\r\n                        Planity\r\n                    ",<br />
  "title": "Lead Javascript Developer"<br />
},</p>
<p>...</p>
<p>]<br />
[/json]</p>
<p>Looks good! However, we haven't told our web spider to follow any links yet.</p>
<p>Right now, it's only pulling the first page of results.</p>
<p>Let's do something about that...</p>
<p>Place the following code <strong>within</strong> your <code>parse</code> function, just below the <strong>for loop</strong>:</p>
<p>[python]<br />
# follow pagination links<br />
next_page = response.css('.pagination .test-pagination-next::attr(href)').extract_first()<br />
  if next_page is not None:<br />
    next_page = response.urljoin(next_page)<br />
    yield scrapy.Request(next_page, callback=self.parse)<br />
[/python]</p>
<p>This code is taken from the <a href="https://doc.scrapy.org/en/1.3/intro/tutorial.html" target="_blank">Scrapy Tutorial</a>, modified for the Stack Overflow site.</p>
<p>If all goes well, this code will tell the crawler to follow the "Next" link at the bottom of each page, and continue yielding results until there are no more result pages.</p>
<p>Let's run the crawler again.  Note: if you output to the same file, it will append results to the end of that file.</p>
<p>So, either delete the json file, or output to a different file name.</p>
<pre>
$ rm stackoverflow.json
$ scrapy crawl stackoverflow -o stackoverflow.json
</pre>
<p>You should have <em>many</em> more results now.</p>
<p>Now it gets really fun! Let's do something with our data!</p>
<p>I've created an Angular app to display the results from multiple sites on one page, and added a search function, too.</p>
<p><img src="images/scrapy-angular-app-300x265.jpg" alt="scrape stack overflow jobs" width="300" height="265" class="aligncenter size-medium wp-image-412" /></p>
<p><a href="https://kimberlythegeek.github.io/scrapy-project/" target="_blank">View on GitHub Pages</a></p>
<p>You can also view the code in my <a href="https://github.com/kimberlythegeek/scrapy-project" target="_blank">GitHub Repository</a>.</p>

  </article>

  <br/>

  
  
</section>

      </div>

      <footer class="footer">
  <section class="container">
     © 2017    ·  Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>. 
  </section>
</footer>

    </main>

    

  </body>

</html>
